{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is meant to familiarize the reader with the sentiment lexicon employed for the tasks of polarity words extraction and key phrase sentiment tagging.\n",
    "# The notebook is organized as follows:\n",
    "- Lexicons employed\n",
    "    - Lexicon 1 generation and preproccessing\n",
    "    - Lexicon 2 generation and preproccessing\n",
    "- Final lexicon generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicons employed\n",
    "Two sentiment lexicons were used to create a combined lexicon for the purposes of this project. Each contains polarity words and their corresponding sentiment on a binary scale of positive or negative. The first one was built by Loughran and McDonald (2013) and the second one by Hu and Liu (2004). The choice of these particular lexicons was determined by Shapiro et al.(2020). In their research, they demonstrate that the combination of these two lexicons seemed to outperform other combinations when classifying financial news sentiment. Despite the fact that the lexicons are employed for different tasks in this project, the same characteristics that render their combination appropriate would remain viable.\n",
    "\n",
    "Below the reader is familiarized with both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon 1 - Loughran and McDonald (2011) (updated 2020)\n",
    "- The lexicon consists of 2707 polarity words extracted from companies' 10-K reports and was originally used in the paper 'When is a Liability Not a Liability? Textual Analysis,Dictionaries, and 10-Ks' (Loughran & McDonald,2011). Since then it has received regular updates and its latest version is the one employed in this project. The lexicon is freely available on [Bill McDonald's page](https://sraf.nd.edu/textual-analysis/resources/). Despit its rather small size, it is particularly suitable for the purposes of this project as word sentiment was specifically considered within the context of the finance/economics domain. Impliedly, the problem of certain words having a different polarity in nonbusiness domains is well addressed and erronous interpretation is significantly reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH= os.path.join('..','external_datasets')\n",
    "SRC = os.path.join(PATH,'LoughranMcDonald_MasterDictionary_2020.csv')\n",
    "master_dictionary= pd.read_csv(SRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The master dictionary file includes various other information that will not be considered for the purposes of this project. Out of the six sentiment categories (negative, positive, uncertainty, litigous, strong_modal, weak_modal), only the negative and positive ones are of interest. Below the reader can see a sample from the original file before preprocessing is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Seq_num</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Strong_Modal</th>\n",
       "      <th>Weak_Modal</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>312</td>\n",
       "      <td>1.422050e-08</td>\n",
       "      <td>1.335201e-08</td>\n",
       "      <td>3.700747e-06</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.367356e-10</td>\n",
       "      <td>8.882163e-12</td>\n",
       "      <td>9.362849e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4.102067e-10</td>\n",
       "      <td>1.200533e-10</td>\n",
       "      <td>5.359747e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>6.836779e-10</td>\n",
       "      <td>4.080549e-10</td>\n",
       "      <td>1.406914e-07</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>5</td>\n",
       "      <td>8009</td>\n",
       "      <td>3.650384e-07</td>\n",
       "      <td>3.798698e-07</td>\n",
       "      <td>3.523914e-05</td>\n",
       "      <td>1058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABACUSES</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABAFT</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1.823141e-10</td>\n",
       "      <td>2.363561e-11</td>\n",
       "      <td>2.491473e-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABALONE</td>\n",
       "      <td>8</td>\n",
       "      <td>140</td>\n",
       "      <td>6.380994e-09</td>\n",
       "      <td>5.055956e-09</td>\n",
       "      <td>1.080602e-06</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ABALONES</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4.557853e-11</td>\n",
       "      <td>8.502178e-11</td>\n",
       "      <td>8.962300e-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>10</td>\n",
       "      <td>118075</td>\n",
       "      <td>5.381685e-06</td>\n",
       "      <td>4.661541e-06</td>\n",
       "      <td>3.338305e-05</td>\n",
       "      <td>62949</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
       "0   AARDVARK        1         312     1.422050e-08        1.335201e-08   \n",
       "1  AARDVARKS        2           3     1.367356e-10        8.882163e-12   \n",
       "2      ABACI        3           9     4.102067e-10        1.200533e-10   \n",
       "3      ABACK        4          15     6.836779e-10        4.080549e-10   \n",
       "4     ABACUS        5        8009     3.650384e-07        3.798698e-07   \n",
       "5   ABACUSES        6           0     0.000000e+00        0.000000e+00   \n",
       "6      ABAFT        7           4     1.823141e-10        2.363561e-11   \n",
       "7    ABALONE        8         140     6.380994e-09        5.055956e-09   \n",
       "8   ABALONES        9           1     4.557853e-11        8.502178e-11   \n",
       "9    ABANDON       10      118075     5.381685e-06        4.661541e-06   \n",
       "\n",
       "        Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
       "0  3.700747e-06         96         0         0            0          0   \n",
       "1  9.362849e-09          1         0         0            0          0   \n",
       "2  5.359747e-08          7         0         0            0          0   \n",
       "3  1.406914e-07         14         0         0            0          0   \n",
       "4  3.523914e-05       1058         0         0            0          0   \n",
       "5  0.000000e+00          0         0         0            0          0   \n",
       "6  2.491473e-08          1         0         0            0          0   \n",
       "7  1.080602e-06         47         0         0            0          0   \n",
       "8  8.962300e-08          1         0         0            0          0   \n",
       "9  3.338305e-05      62949      2009         0            0          0   \n",
       "\n",
       "   Strong_Modal  Weak_Modal  Constraining  Complexity  Syllables     Source  \n",
       "0             0           0             0           0          2  12of12inf  \n",
       "1             0           0             0           0          2  12of12inf  \n",
       "2             0           0             0           0          3  12of12inf  \n",
       "3             0           0             0           0          2  12of12inf  \n",
       "4             0           0             0           0          3  12of12inf  \n",
       "5             0           0             0           0          4  12of12inf  \n",
       "6             0           0             0           0          2  12of12inf  \n",
       "7             0           0             0           0          4  12of12inf  \n",
       "8             0           0             0           0          4  12of12inf  \n",
       "9             0           0             0           0          3  12of12inf  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_dictionary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polarity of words is determined by the year tag (the year when the word was added to the lexicon) in the respective column. For example, above, one can see that 'abandon' was added in 2009 and it is classified as negative. Below, the master dictionary file is processed to include only polarity words which have been tagged as either negative or positive. \n",
    "\n",
    "The daframe must be converted to contain two columns - word and sentiment. In order to do so, two separate dataframes for the positive and negative words are created before merging them in one. Below, the steps taken to do so are illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABLE</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABUNDANCE</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABUNDANT</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACCLAIMED</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACCOMPLISH</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>WRONGDOING</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>WRONGDOINGS</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>WRONGFUL</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>WRONGFULLY</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>WRONGLY</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2709 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word sentiment\n",
       "0            ABLE  positive\n",
       "1       ABUNDANCE  positive\n",
       "2        ABUNDANT  positive\n",
       "3       ACCLAIMED  positive\n",
       "4      ACCOMPLISH  positive\n",
       "...           ...       ...\n",
       "2704   WRONGDOING  negative\n",
       "2705  WRONGDOINGS  negative\n",
       "2706     WRONGFUL  negative\n",
       "2707   WRONGFULLY  negative\n",
       "2708      WRONGLY  negative\n",
       "\n",
       "[2709 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Create a pandas DataFrame consisting only of columns 'Word' and 'Positive' and replace all 0s with NaN to\n",
    "ease removing these observations. Following the described operation, only the positive words are left and column 'Positive' is removed\n",
    "and replaced by column sentiment which is subsequently populated with the sentiment class - positive. The same procedure is \n",
    "repeated for the negative words in the master dictionary dataframe. Lastly, the two dataframes are merged to generate \n",
    "the final Loughran & McDonald lexicon dataframe.\"\"\"\n",
    "\n",
    "# Generate the dataframe consisting of columns 'Word' and 'Positive'\n",
    "positive_df=master_dictionary[['Word','Positive']]\n",
    "# Replace all 0s with NaN and drop all observations having that value in the 'Positive' column. The column is subsequently removed.\n",
    "positive_df=positive_df.replace(0,np.nan).dropna(subset=['Positive']).iloc[::,:1]\n",
    "# Column sentiment is added and populated with 'positive'\n",
    "positive_df['sentiment']='positive'\n",
    "\n",
    "# Generate the dataframe consisting of columns 'Word' and 'Negative'\n",
    "negative_df=master_dictionary[['Word','Negative']]\n",
    "# Replace all 0s with NaN and drop all observations having that value in the 'Positive' column. The column is subsequently removed.\n",
    "negative_df=negative_df.replace(0,np.nan).dropna(subset=['Negative']).iloc[::,:1]\n",
    "# Column sentiment is added and populated with 'negative'\n",
    "negative_df['sentiment']='negative'\n",
    "\n",
    "# Merge the positive and negative dataframes to generate the final Loughran & McDonald lexicon.\n",
    "LM_lexicon=positive_df.append(negative_df,ignore_index=True)\n",
    "# Lowercase the 'Word' column name\n",
    "LM_lexicon.columns=['word','sentiment']\n",
    "LM_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the reader can observe the label distribution. The distribution is heavily skewed towards negative words with them accouting for close to 87% of all words in the lexicon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2355\n",
       "positive     354\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LM_lexicon.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon 2 - Hu and Liu (2004)\n",
    "- Hu and Liu developed their lexicon using movie reviews. The lexicon contains close to 6,800 sentiment words classified on a binary scale of negative or positive. Its size makes it a useful contribution to Lougran and Mcdonald's lexicon when generating the sentiment lexicon used in this project. The lexicon is freely distributed on [Bing Liu's page](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a+</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abound</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abounds</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abundance</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abundant</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6784</th>\n",
       "      <td>zaps</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6785</th>\n",
       "      <td>zealot</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>zealous</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6787</th>\n",
       "      <td>zealously</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788</th>\n",
       "      <td>zombie</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6789 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word sentiment\n",
       "0            a+  positive\n",
       "1        abound  positive\n",
       "2       abounds  positive\n",
       "3     abundance  positive\n",
       "4      abundant  positive\n",
       "...         ...       ...\n",
       "6784       zaps  negative\n",
       "6785     zealot  negative\n",
       "6786    zealous  negative\n",
       "6787  zealously  negative\n",
       "6788     zombie  negative\n",
       "\n",
       "[6789 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_POS = os.path.join(PATH,'Hu_Liu_positive.txt')\n",
    "SRC_NEG=os.path.join(PATH,'Hu_Liu_negative.txt')\n",
    "\n",
    "\"\"\"The two separate txt files containing the positive and negative words are processed as separate dataframes. \n",
    "Column headers are added to both and their sentiment is populated before merging them into one dataframe.\"\"\"\n",
    "\n",
    "# Create the positive words dataframe\n",
    "positive_words=pd.read_csv(SRC_POS,header=None,names=['word','sentiment'])\n",
    "positive_words['sentiment']='positive'\n",
    "# Create the negative words dataframe\n",
    "negative_words=pd.read_csv(SRC_NEG,header=None,names=['word','sentiment'])\n",
    "negative_words['sentiment']='negative'\n",
    "\n",
    "# Merge both to create the final Hu and Liu lexicon dataframe\n",
    "HL_lexicon=positive_words.append(negative_words,ignore_index=True)\n",
    "HL_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the reader can observe the label distribution. Again, the distribution is skewed towards negative words that account for 70% of the polarity words present in the lexicon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    4783\n",
       "positive    2006\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HL_lexicon.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final lexicon generation\n",
    "- The final lexicon will consist of both Loughran and McDonald's and Hu and Liu's lexicons merged. As mentioned, the reasoning for the choice of that combination was derived from Shapiro et al.(2020). Since Loughran and McDonald's lexicon is specifically targeted for the finance/economics domain, should there be any duplicated values, the observation from Loughran and McDonald's lexicon will be kept and the one from Hu and Liu's removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>able</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abundance</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abundant</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acclaimed</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accomplish</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6784</th>\n",
       "      <td>zaps</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6785</th>\n",
       "      <td>zealot</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>zealous</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6787</th>\n",
       "      <td>zealously</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788</th>\n",
       "      <td>zombie</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8453 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word sentiment\n",
       "0           able  positive\n",
       "1      abundance  positive\n",
       "2       abundant  positive\n",
       "3      acclaimed  positive\n",
       "4     accomplish  positive\n",
       "...          ...       ...\n",
       "6784        zaps  negative\n",
       "6785      zealot  negative\n",
       "6786     zealous  negative\n",
       "6787   zealously  negative\n",
       "6788      zombie  negative\n",
       "\n",
       "[8453 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge both lexicons to create the final sentiment lexicon\n",
    "sentiment_lexicon=LM_lexicon.append(HL_lexicon)\n",
    "# Lowercase all words\n",
    "sentiment_lexicon[\"word\"]=sentiment_lexicon[\"word\"].apply(lambda x: x.lower())\n",
    "# Drop any duplicates, keeping the first observation which will be the word from Lougran and McDonald's lexicon.\n",
    "sentiment_lexicon=sentiment_lexicon.drop_duplicates(subset=\"word\",keep=\"first\")  \n",
    "\n",
    "\n",
    "sentiment_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    6294\n",
       "positive    2159\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_lexicon.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = os.path.join('..','project_datasets')\n",
    "path = os.path.join(SRC,'sentiment_lexicon.csv')\n",
    "sentiment_lexicon.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliography\n",
    "\n",
    "Shapiro, A.H., Sudhof, M. and Wilson, D.J., 2020. Measuring news sentiment. Journal of Econometrics.\n",
    "\n",
    "[Loughran, T. and McDonald, B., 2011. When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks. The Journal of finance, 66(1), pp.35-65.](https://sraf.nd.edu/textual-analysis/resources/)\n",
    "\n",
    "[Hu, M., and B. Liu (2004): “Mining and summarizing customer reviews,” in SIGKDDKDM-04.](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
