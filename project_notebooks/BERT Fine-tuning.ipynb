{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oubqktjSKc2Y",
    "outputId": "77830de8-557d-4004-b256-713e260c1231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check whether GPU is successfully acquired\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    #Set device used to be the GPU if available  \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU found:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('GPU could not connect')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXUv60seNfKR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "!pip install transformers\n",
    "from transformers import BertTokenizer,BertForSequenceClassification,AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvlN8SGMznPH"
   },
   "outputs": [],
   "source": [
    "#Fix random seeds so that results can be reproduced\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMAZ_rH0Mutx",
    "outputId": "deac675e-9e27-41e9-bb7b-1669296bea79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount a drive where the model will be exported later on\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLBLhnLIvJYG"
   },
   "source": [
    "## This notebook is meant to familiarize the reader with the process fine-tuning the pre-trained Bidirectional Encoder Representations from Transformers(BERT) model originally developed by Devlin, et al.(2018). \n",
    "\n",
    "### The notebook is structured as follows:\n",
    "1. General Information\n",
    "  1. Bidirectional Encoder Representations from Transformers (BERT) Model\n",
    "  1. BERT Model ImplementatioN\n",
    "  1. Fine-tuning Process Overview\n",
    " \n",
    "1. Brief Dataset Overview (the reader can get more detailed information about the dataset in the 'Model Data Generation and Exploration' notebook).\n",
    "1. Creating a Custom Dataset Class as Required by Pytorch.\n",
    "1. Generating Train and Test Splits.\n",
    "1. Training Proccess \n",
    " 1. Choice of Hyperparameters\n",
    " 1. Choice of Evaluation Metrics \n",
    " 1. Training Procedure\n",
    " 1. Evaluation Procedure\n",
    "2. Exported Model Testing Procedure\n",
    "  \n",
    "\n",
    "Detailed description is provided in each subpoint.\n",
    "\n",
    "- **Important notes about recreating results**: \n",
    " - Should the reader wish to recreate the training procedure, a **Tesla T4** (or better) GPU must be used. GPUs having less processing power will likely render training not possible due to memory constraints.\n",
    " - Fine-tuning was done using Google Colab hence some cells contain commands specific to the Colab environment (e.g mounting Google drive and importing the dataset). These would not be needed in a normal jupyter notebook or other environment and the reader can use the dataset found in the project_dataset direcory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "ZOTPxwtEMtIR",
    "outputId": "77259334-47a0-4d18-8614-e269eb65074c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-55fd391c-e47c-46ac-ba88-d07ae693235e\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-55fd391c-e47c-46ac-ba88-d07ae693235e\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving bert_dataset.csv to bert_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Upload the dataset exported from the 'Model Data Generation and Exploration' notebook.\n",
    "from google.colab import files\n",
    "import io\n",
    "uploaded = files.upload()\n",
    "train_data = pd.read_csv(io.BytesIO(uploaded['bert_dataset.csv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npYVjqDezGj3"
   },
   "source": [
    "## General Information\n",
    "\n",
    "### Bidirectional Encoder Representations from Transformers (BERT) Model\n",
    "\n",
    "- BERT was trained on a large corpus of texts from Wikipedia and the BooksCorpus. The model uses two mechanisms for error signalling - masked languge modelling (Masked LM) and next sentence prediction (NSP). The former involves replacing 15% of the words in any sequence by a [MASK] token and letting the model predict it. The latter technique implies replacing mixing sequential sentences 50% of the time and allowing the model to predict whether the second sentence follows from the previous one or if it is a random replacement. The model architecture builds upon the seminal Transformer network model (Vaswani, et al.,2017) and consists of 12 transformer blocks, 768 hidden layers, 12 attention heads, and 110 million parameters. The authors of the original paper further propose an architecture that has twice as many of each of the above described features but it will not be considered due to its resource requirements that would render it impossible to use. Each word in any sequence is represented by a 768-dimensional vector and BERT also includes what is refferred to as special tokens that are meant for usage in downstream tasks. The [CLS] special token is the one employed for classification and it essentially serves as a summary representation of an entire sequence.\n",
    "\n",
    "### BERT Implementation\n",
    "\n",
    "- The Hugging Face library provides various implementations of the BERT model [(Hugging Face.,*BERT*, 2021)](https://huggingface.co/transformers/model_doc/bert.html). For the purposes of this project the BERT base model Pytorch implementation was used [(Hugging Face,*bert-base-cased* 2021)](https://huggingface.co/bert-base-cased). The architecture consists of 12 transformer blocks(768 hidden layers), 12 attention heads, and 110 million parameters. The library also offers many options for pre-trained weights, and bert-base-cased was the one chosen. Additionally, since in this case BERT will be used for a downstream task, the Hugging Face library provides a model that includes an extra fully connected linear layer that can be trained for the task of sentiment classification - BertForSequenceClassification [(Hugging Face.,*BertForSequenceClassification*, 2021)](https://https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification). This classification head is built on top of the [CLS] pooled output mentioned above. The library also provides text tokenizers for each of the pre-trained models they offer [(Hugging Face.,*Tokenizer*, 2021)](https://huggingface.co/transformers/main_classes/tokenizer.html).\n",
    "\n",
    "### Fine-tuning process overview\n",
    "- During the process of fine-tuning the model is first initialized with its pre-trained parameters. The embedding weights are adjusted through BERT's established learning mechansims - masked LM and NSP. The weights of the ouput layer for the given task, in this case sentiment classification, are learned using the labelled data. In that manner the model is trained end-to-end for the given task (Devlin, et al.,2018).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdJevsuDUQ-o"
   },
   "source": [
    "## Brief Dataset Overview (refer to the 'Model Data Generation and Exploration' notebook for detailed dataset information)\n",
    "\n",
    "- The dataset contains financial news headlines headline texts and their sentiment annotation on a ternary scale of negative/neutral/positive.\n",
    "Two existing datsets were combined to generate the final set used - [Financial Phrase Bank (Malo, P. et al.,2013)](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) and data by [Sousa et al.(2019)](https://github.com/stocks-predictor/bert) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gcm3heAHPPmr",
    "outputId": "3877ea44-303d-43d6-8bcf-39bac4632912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains a total of 5946 sentences\n",
      "Below one can see the label distribution between the three sentiment categories:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "neutral     3418\n",
       "positive    1574\n",
       "negative     954\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The dataset contains a total of {} sentences'.format(train_data.shape[0]))\n",
    "\n",
    "print('Below one can see the label distribution between the three sentiment categories:')\n",
    "labels_map={'label':{0:'negative',1:'neutral',2:'positive'}}\n",
    "train_data.replace(labels_map).label.value_counts()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHU-fvDUzq64"
   },
   "source": [
    "## Generating the Train/Test Split\n",
    "\n",
    "- A 80/20 proportion split was used. The training dataset will be used for training the model using 10-fold cross validation.\n",
    "  The model that generates the higher macro avg f1 is the one that will be exported for use in the application. Therefore, in order to validate its ability to generalize \n",
    "    well on unseen data, the test split will be used. In that manner, the evaluation metrics produced during the training folds will provide evidence for the correctness of hyperparameter choices and\n",
    "    the evaluation metrics provided from testing the exported model with the test data will showcase the performance of the model used for the application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgqrnEXFXV_T"
   },
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_data.headline, train_data.label, test_size=.2,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta6C1h10VvQK"
   },
   "source": [
    "## Creating a Custom Dataset Class as Required by Pytorch\n",
    "- The dataset class extends pytorch.Dataset and implements the requiered methods [(Pytorch., *Data Loading Tutorial*, 2021)](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) [(Hugging Face., *Fine-tuning a Pre-trained Model* 2021)](https://huggingface.co/transformers/training.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50NJD7DtYILJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  \"\"\"Params:\n",
    "      encodings: transformers.tokenization_utils_base.BatchEncoding - the encoding returned by BertTokenizer.encode_plus\n",
    "      labels: list - list of labels for the dataset \n",
    "      raw_text: list - list of the raw examples from the dataset \n",
    "    \"\"\"\n",
    "  def __init__(self, encodings, labels,raw_text):\n",
    "      self.encodings = encodings\n",
    "      self.labels = labels\n",
    "      self.raw_text=raw_text\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "      item['labels'] = torch.tensor(self.labels[idx])\n",
    "      return item\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7F9pE3UX6Nw"
   },
   "source": [
    "## Training Proccess Details\n",
    "- Due to the rather small size of the dataset employed for training, a decision was made to use 10-fold cross-validation in order to best estimate the model's performance and reduce bias as much as possible. More specifically, due to the label imbalance of the dataset, stratified k-fold cross validation was employed implemented using the scikit-learn library(Pedregosa et al.,2011). Using stratified instead of regular k-fold results in a more balanced sample of labels between each split instead of randomly selecting observations which could lead to distortions in evaluating performance.  The model that scores the highest macro average f1 score will be saved for further evaluation. The training proccess will be repeated multiple times and then the performance of some of the different saved models will be compared to select the final one that will be used for the application. The capabilities of the different models will be judged by their ability to generalize well on the test portion of the dataset and also how they perform on scoring the sentiment of the articles dataset specifically built for this project (see the 'Selecting Model for Production' notebook). The articles dataset contains entire news articles texts and controls for the model's performance when the extra logic used to arrive at a final sentiment classification is also used.\n",
    "\n",
    "### Choice of Hyperparameters\n",
    "- Devlin et al., (2018) advice for a choice between any combination of the following hyperparameters:\n",
    " - Batch size:16,32\n",
    " - Learning rate (Adam): 5e-5,3e-5,2e-5\n",
    " - Number of epochs: 2,3,4\n",
    " \n",
    " Having run multiple runs with different combinations of the above, the final selection is as follows: \n",
    "  - Batch size: 32\n",
    "  - Learning rate: 2e-5\n",
    "  - Number of epochs: 4\n",
    "\n",
    "The rather large choice of batch size ensures that training is completed within reasonble time constraints. It's potenital detrimental impact on generalization is tackled by a smaller learning rate, using gradient clipping and learning rate decay (Hoffer, Hubara, & Soudry, 2017). Furthermore, the use of gradient clipping also reduces the possibility of the exploding gradient problem.\n",
    "\n",
    "### Choice of Evaluation Metrics\n",
    "- The model's performance is evaluated using the macro averaged f1-score. The reason for deeming other evaluation metrics like accuracy, precision, recall or simple f1-score not appropriate stems from the label imbalance of\n",
    " the dataset used. Additionally, equal importance of each class should be attributed hence avoiding using just the f1-score. The macro average f1-score expands on the simple f1-score by averaging the f1-score for each class. Impliedly, it should communicate both information about the true positive rate (recall) and incorrect predictions (precision) while avoiding the pitfall of using these metrics in a stand-alone fashion which could be deceiving due to the mentioned characteristics of the dataset. One potential weakness of the macro averaged f1-score is that it could overestimate the model's performance in cases where it performs really well in classifying a given smaller class but not as good in others. \n",
    "\n",
    " ### Training Procedure\n",
    "- The training split consisting of 80% of the observations of the dataset is used for training. As mentioned stratified 10-fold cross-validation is used to assess the performance of the model and the one that scores best(per macro averaged f1-score) is exported to be used for predictions in the application. After each fold the previously model that was trained for 4 epochs is removed from GPU memory to avoid 'out of memory' exceptions.\n",
    "- Steps:\n",
    "  - Generate the stratified k-fold splits, with k=10.\n",
    "  - Generate the BERT encodings that are required by BERT using BertTokenizer. The encodings per observation consist of the following:\n",
    "   - input ids: The ids of each token in the sequence including word tokens ids and special tokens ids. As the model takes fixed length input id tensors, each tensor is padded to a given length passed as a parameter which should not exceed the maximum allowed of 512. A factor that was considered when selecting the token length was the complexity of the model which grows quadratic with the increase of sequence length. Therefore, a maximum length of 300 was chosen as a balancing compromise between allowing the model to learn longer dependencies while maintaining computational costs at a reasonable level. \n",
    "   - attention mask: A tensor of length equal to the maximum length(300) which indicates which ids in input ids are padding tokens and which tokens should the model consider.\n",
    "- Create the pytorch datasets and dataloaders using batch size of 32 for the dataloaders.\n",
    "-Instantiate the model, Adam optimizer(used for adjusting the model weights) and learning rate decay scheduler. The learning rate is set at 2e-5 and it will be linearly decayed.\n",
    "-Run an epoch (**more details are provided as comments in the code section**)\n",
    "  - For each epoch a training and validation loops are ran. After each validation loop a classification report is generated and the model with highest macro averaged f1-score is saved.\n",
    "\n",
    " ### Evaluation Procedure\n",
    "- After all folds are concluded, the macro average f1-score for each epoch in each fold is averaged to produce the final evaluation metric for the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_onYpGYvLF83"
   },
   "outputs": [],
   "source": [
    "''' As mentioned differrent models will be evaluated. For one of the models the dataset will be preprocessed to control for any punctuation irregularrities.\n",
    "    The reason for using such an approach is that accorrding to Devlin, et al. (2018), the original model was trained on large text corpuses\n",
    "    without any punctuation preprocessing. Therefore, in order to fully recreate the format of the data used for generating the original weights,\n",
    "    the below two functions were used for preproccessing the dataset. They will only be used for one of the exported models - BertFixedPunct.\n",
    "'''\n",
    "def punct_recover(text):\n",
    "    return text.replace(\" .\", \".\").replace(\" 're\", \"'re'\").replace(\" ,\", \",\").replace(\" 's\", \"'s\").replace(\" 've\", \"'ve\").replace(\" 't\", \"'t\").replace(\" 'd\", \"'d\").replace(\" %\",\"%\")\n",
    "\n",
    "def restore_punct(df):\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        df.sentence.iloc[i]=punct_recover(df.sentence.iloc[i])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FryiyhhqiPcD"
   },
   "outputs": [],
   "source": [
    "# Instantiate the bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "fb67a4f43df74a7483a57b7b52a3d650",
      "469ad9498e3e49d49b84f3412e53a4f6",
      "62b574378c88496e9351cb386c09f547",
      "8803f3d2edaa4a6eb3219c48d3f27131",
      "51abd8db8f7d4638b72c25c3df51d099",
      "c3c1ec160566433794e2789673779bc2",
      "97b9c3ddc9bc456a8291bcd625164359",
      "33dc6b8de24841318364b3dc93b2c389",
      "dbae98c03303407090ab80e1077b47db",
      "989e6f80a0d94cc6a3873f883d330720",
      "eeba3980337444968a1b5714f59a3910"
     ]
    },
    "id": "6fqbEB5jcepJ",
    "outputId": "2810c843-533f-430c-e8d9-79f3ab16b53d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============Fold: 1 of 10 ============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb67a4f43df74a7483a57b7b52a3d650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 1==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.7010188439666335\n",
      "============Validation pass of epoch 1 , fold 1 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.69      0.75        55\n",
      "     neutral       0.95      0.70      0.80       286\n",
      "    positive       0.57      0.93      0.71       135\n",
      "\n",
      "    accuracy                           0.76       476\n",
      "   macro avg       0.78      0.77      0.75       476\n",
      "weighted avg       0.82      0.76      0.77       476\n",
      "\n",
      "Saving model...\n",
      "Model saved!!\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:48.439065.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 1==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.30660570935526893\n",
      "============Validation pass of epoch 2 , fold 1 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.78      0.78        55\n",
      "     neutral       0.96      0.76      0.85       286\n",
      "    positive       0.63      0.91      0.74       135\n",
      "\n",
      "    accuracy                           0.80       476\n",
      "   macro avg       0.79      0.82      0.79       476\n",
      "weighted avg       0.85      0.80      0.81       476\n",
      "\n",
      "Saving model...\n",
      "Model saved!!\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:47.187258.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 1==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.17248693882807423\n",
      "============Validation pass of epoch 3 , fold 1 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.78      0.77        55\n",
      "     neutral       0.97      0.68      0.80       286\n",
      "    positive       0.57      0.93      0.71       135\n",
      "\n",
      "    accuracy                           0.76       476\n",
      "   macro avg       0.77      0.80      0.76       476\n",
      "weighted avg       0.83      0.76      0.77       476\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.481794.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 1==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.10969010679357087\n",
      "============Validation pass of epoch 4 , fold 1 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.76      0.79        55\n",
      "     neutral       0.96      0.73      0.83       286\n",
      "    positive       0.60      0.93      0.73       135\n",
      "\n",
      "    accuracy                           0.79       476\n",
      "   macro avg       0.80      0.81      0.78       476\n",
      "weighted avg       0.84      0.79      0.80       476\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:45.217019.=============\n",
      "============Fold 1 completed. It took 0:15:25.543617.========\n",
      "=============Fold: 2 of 10 ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 2==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.5995002613583608\n",
      "============Validation pass of epoch 1 , fold 2 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.74      0.85        54\n",
      "     neutral       0.95      0.82      0.88       286\n",
      "    positive       0.71      0.99      0.83       136\n",
      "\n",
      "    accuracy                           0.86       476\n",
      "   macro avg       0.89      0.85      0.85       476\n",
      "weighted avg       0.89      0.86      0.86       476\n",
      "\n",
      "Saving model...\n",
      "Model saved!!\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:46.981512.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 2==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.29704392873751584\n",
      "============Validation pass of epoch 2 , fold 2 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.78      0.86        54\n",
      "     neutral       0.95      0.86      0.90       286\n",
      "    positive       0.77      0.99      0.87       136\n",
      "\n",
      "    accuracy                           0.89       476\n",
      "   macro avg       0.89      0.87      0.88       476\n",
      "weighted avg       0.90      0.89      0.89       476\n",
      "\n",
      "Saving model...\n",
      "Model saved!!\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:47.061882.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 2==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.1819175233536247\n",
      "============Validation pass of epoch 3 , fold 2 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.76      0.85        54\n",
      "     neutral       0.95      0.87      0.91       286\n",
      "    positive       0.77      0.99      0.86       136\n",
      "\n",
      "    accuracy                           0.89       476\n",
      "   macro avg       0.90      0.87      0.88       476\n",
      "weighted avg       0.90      0.89      0.89       476\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.444413.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 2==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.11788967162815493\n",
      "============Validation pass of epoch 4 , fold 2 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.78      0.86        54\n",
      "     neutral       0.95      0.86      0.90       286\n",
      "    positive       0.77      0.99      0.86       136\n",
      "\n",
      "    accuracy                           0.88       476\n",
      "   macro avg       0.89      0.87      0.87       476\n",
      "weighted avg       0.90      0.88      0.89       476\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:45.413224.=============\n",
      "============Fold 2 completed. It took 0:15:09.321621.========\n",
      "=============Fold: 3 of 10 ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 3==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.6360752738233822\n",
      "============Validation pass of epoch 1 , fold 3 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.37      0.51        54\n",
      "     neutral       0.91      0.75      0.82       286\n",
      "    positive       0.58      0.93      0.71       136\n",
      "\n",
      "    accuracy                           0.76       476\n",
      "   macro avg       0.77      0.68      0.68       476\n",
      "weighted avg       0.81      0.76      0.76       476\n",
      "\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:45.437006.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 3==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.31726629637292963\n",
      "============Validation pass of epoch 2 , fold 3 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.74      0.81        54\n",
      "     neutral       0.89      0.88      0.88       286\n",
      "    positive       0.73      0.79      0.76       136\n",
      "\n",
      "    accuracy                           0.84       476\n",
      "   macro avg       0.84      0.80      0.82       476\n",
      "weighted avg       0.84      0.84      0.84       476\n",
      "\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:45.669095.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 3==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.18751981159421935\n",
      "============Validation pass of epoch 3 , fold 3 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.65      0.76        54\n",
      "     neutral       0.92      0.82      0.87       286\n",
      "    positive       0.66      0.88      0.75       136\n",
      "\n",
      "    accuracy                           0.82       476\n",
      "   macro avg       0.83      0.78      0.79       476\n",
      "weighted avg       0.84      0.82      0.82       476\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.704798.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 3==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.11475994841042739\n",
      "============Validation pass of epoch 4 , fold 3 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.67      0.77        54\n",
      "     neutral       0.90      0.83      0.87       286\n",
      "    positive       0.67      0.86      0.75       136\n",
      "\n",
      "    accuracy                           0.82       476\n",
      "   macro avg       0.83      0.79      0.80       476\n",
      "weighted avg       0.84      0.82      0.82       476\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:45.743422.=============\n",
      "============Fold 3 completed. It took 0:15:06.913087.========\n",
      "=============Fold: 4 of 10 ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 4==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.5807875553841022\n",
      "============Validation pass of epoch 1 , fold 4 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.80      0.68        54\n",
      "     neutral       0.94      0.88      0.91       286\n",
      "    positive       0.72      0.73      0.73       136\n",
      "\n",
      "    accuracy                           0.83       476\n",
      "   macro avg       0.75      0.80      0.77       476\n",
      "weighted avg       0.84      0.83      0.83       476\n",
      "\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:45.774664.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 4==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.2946066168039592\n",
      "============Validation pass of epoch 2 , fold 4 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.80      0.70        54\n",
      "     neutral       0.96      0.85      0.90       286\n",
      "    positive       0.69      0.79      0.73       136\n",
      "\n",
      "    accuracy                           0.82       476\n",
      "   macro avg       0.76      0.81      0.78       476\n",
      "weighted avg       0.85      0.82      0.83       476\n",
      "\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:45.790192.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 4==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.17639630999582917\n",
      "============Validation pass of epoch 3 , fold 4 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.80      0.71        54\n",
      "     neutral       0.94      0.90      0.92       286\n",
      "    positive       0.75      0.75      0.75       136\n",
      "\n",
      "    accuracy                           0.84       476\n",
      "   macro avg       0.78      0.81      0.79       476\n",
      "weighted avg       0.85      0.84      0.85       476\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.773231.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 4==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.11441038047144217\n",
      "============Validation pass of epoch 4 , fold 4 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.80      0.71        54\n",
      "     neutral       0.94      0.90      0.92       286\n",
      "    positive       0.76      0.75      0.75       136\n",
      "\n",
      "    accuracy                           0.84       476\n",
      "   macro avg       0.78      0.81      0.79       476\n",
      "weighted avg       0.85      0.84      0.85       476\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:45.511314.=============\n",
      "============Fold 4 completed. It took 0:15:07.034839.========\n",
      "=============Fold: 5 of 10 ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 5==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.7789658548226998\n",
      "============Validation pass of epoch 1 , fold 5 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        54\n",
      "     neutral       0.80      0.87      0.83       286\n",
      "    positive       0.63      0.78      0.70       136\n",
      "\n",
      "    accuracy                           0.74       476\n",
      "   macro avg       0.48      0.55      0.51       476\n",
      "weighted avg       0.66      0.74      0.70       476\n",
      "\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:45.655010.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 5==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.49706809000292823\n",
      "============Validation pass of epoch 2 , fold 5 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.48      0.58        54\n",
      "     neutral       0.79      0.94      0.86       286\n",
      "    positive       0.88      0.62      0.73       136\n",
      "\n",
      "    accuracy                           0.80       476\n",
      "   macro avg       0.80      0.68      0.72       476\n",
      "weighted avg       0.81      0.80      0.79       476\n",
      "\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:45.771818.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 5==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.31316462120235855\n",
      "============Validation pass of epoch 3 , fold 5 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.61      0.70        54\n",
      "     neutral       0.84      0.93      0.88       286\n",
      "    positive       0.86      0.76      0.80       136\n",
      "\n",
      "    accuracy                           0.84       476\n",
      "   macro avg       0.84      0.77      0.80       476\n",
      "weighted avg       0.84      0.84      0.84       476\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.683045.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 5==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.22985597013203957\n",
      "============Validation pass of epoch 4 , fold 5 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.56      0.65        54\n",
      "     neutral       0.83      0.94      0.88       286\n",
      "    positive       0.88      0.74      0.80       136\n",
      "\n",
      "    accuracy                           0.84       476\n",
      "   macro avg       0.83      0.74      0.78       476\n",
      "weighted avg       0.84      0.84      0.83       476\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:45.842837.=============\n",
      "============Fold 5 completed. It took 0:15:07.339217.========\n",
      "=============Fold: 6 of 10 ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 6==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.6050602231007903\n",
      "============Validation pass of epoch 1 , fold 6 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.80      0.88        55\n",
      "     neutral       0.85      0.95      0.90       285\n",
      "    positive       0.88      0.72      0.79       136\n",
      "\n",
      "    accuracy                           0.87       476\n",
      "   macro avg       0.90      0.82      0.86       476\n",
      "weighted avg       0.87      0.87      0.87       476\n",
      "\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:45.747169.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 6==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.3038130418689393\n",
      "============Validation pass of epoch 2 , fold 6 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.84      0.88        55\n",
      "     neutral       0.87      0.94      0.91       285\n",
      "    positive       0.86      0.75      0.80       136\n",
      "\n",
      "    accuracy                           0.88       476\n",
      "   macro avg       0.89      0.84      0.86       476\n",
      "weighted avg       0.88      0.88      0.87       476\n",
      "\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:45.858653.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 6==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.17585835047066212\n",
      "============Validation pass of epoch 3 , fold 6 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.91      0.90        55\n",
      "     neutral       0.88      0.93      0.90       285\n",
      "    positive       0.87      0.76      0.82       136\n",
      "\n",
      "    accuracy                           0.88       476\n",
      "   macro avg       0.88      0.87      0.87       476\n",
      "weighted avg       0.88      0.88      0.88       476\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.835421.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 6==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.10270369936253375\n",
      "============Validation pass of epoch 4 , fold 6 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.87      0.89        55\n",
      "     neutral       0.88      0.92      0.90       285\n",
      "    positive       0.86      0.79      0.82       136\n",
      "\n",
      "    accuracy                           0.88       476\n",
      "   macro avg       0.88      0.86      0.87       476\n",
      "weighted avg       0.88      0.88      0.88       476\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:45.715729.=============\n",
      "============Fold 6 completed. It took 0:15:07.389213.========\n",
      "=============Fold: 7 of 10 ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 7==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.5827652277119124\n",
      "============Validation pass of epoch 1 , fold 7 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.91      0.88        55\n",
      "     neutral       0.83      0.93      0.88       285\n",
      "    positive       0.87      0.62      0.72       135\n",
      "\n",
      "    accuracy                           0.84       475\n",
      "   macro avg       0.85      0.82      0.83       475\n",
      "weighted avg       0.84      0.84      0.83       475\n",
      "\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:46.033426.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 7==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.28885243099127245\n",
      "============Validation pass of epoch 2 , fold 7 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.71      0.82        55\n",
      "     neutral       0.79      0.96      0.87       285\n",
      "    positive       0.83      0.55      0.66       135\n",
      "\n",
      "    accuracy                           0.81       475\n",
      "   macro avg       0.87      0.74      0.78       475\n",
      "weighted avg       0.82      0.81      0.80       475\n",
      "\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:45.830641.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 7==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.17513962744721281\n",
      "============Validation pass of epoch 3 , fold 7 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.82      0.87        55\n",
      "     neutral       0.85      0.94      0.90       285\n",
      "    positive       0.86      0.70      0.78       135\n",
      "\n",
      "    accuracy                           0.86       475\n",
      "   macro avg       0.88      0.82      0.85       475\n",
      "weighted avg       0.86      0.86      0.86       475\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.935961.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 7==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.10690968066676339\n",
      "============Validation pass of epoch 4 , fold 7 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.85      0.90        55\n",
      "     neutral       0.83      0.95      0.89       285\n",
      "    positive       0.87      0.63      0.73       135\n",
      "\n",
      "    accuracy                           0.85       475\n",
      "   macro avg       0.88      0.81      0.84       475\n",
      "weighted avg       0.85      0.85      0.84       475\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:45.849923.=============\n",
      "============Fold 7 completed. It took 0:15:08.009047.========\n",
      "=============Fold: 8 of 10 ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 8==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.5614181299930188\n",
      "============Validation pass of epoch 1 , fold 8 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.89      0.77        55\n",
      "     neutral       0.89      0.85      0.87       285\n",
      "    positive       0.76      0.74      0.75       135\n",
      "\n",
      "    accuracy                           0.82       475\n",
      "   macro avg       0.77      0.83      0.79       475\n",
      "weighted avg       0.83      0.82      0.82       475\n",
      "\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:45.639329.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 8==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.26506515636817735\n",
      "============Validation pass of epoch 2 , fold 8 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.84      0.80        55\n",
      "     neutral       0.85      0.92      0.88       285\n",
      "    positive       0.83      0.67      0.74       135\n",
      "\n",
      "    accuracy                           0.84       475\n",
      "   macro avg       0.82      0.81      0.81       475\n",
      "weighted avg       0.84      0.84      0.83       475\n",
      "\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:45.849264.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 8==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.1603585375306099\n",
      "============Validation pass of epoch 3 , fold 8 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.87      0.81        55\n",
      "     neutral       0.88      0.89      0.89       285\n",
      "    positive       0.82      0.74      0.78       135\n",
      "\n",
      "    accuracy                           0.85       475\n",
      "   macro avg       0.82      0.84      0.82       475\n",
      "weighted avg       0.85      0.85      0.85       475\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.837987.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 8==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.09353570381540861\n",
      "============Validation pass of epoch 4 , fold 8 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.89      0.78        55\n",
      "     neutral       0.88      0.89      0.88       285\n",
      "    positive       0.81      0.70      0.75       135\n",
      "\n",
      "    accuracy                           0.84       475\n",
      "   macro avg       0.79      0.83      0.81       475\n",
      "weighted avg       0.84      0.84      0.84       475\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:45.449158.=============\n",
      "============Fold 8 completed. It took 0:15:06.976815.========\n",
      "=============Fold: 9 of 10 ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 9==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.606054838246374\n",
      "============Validation pass of epoch 1 , fold 9 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.85      0.70        55\n",
      "     neutral       0.88      0.93      0.90       285\n",
      "    positive       0.74      0.52      0.61       135\n",
      "\n",
      "    accuracy                           0.80       475\n",
      "   macro avg       0.74      0.77      0.74       475\n",
      "weighted avg       0.80      0.80      0.79       475\n",
      "\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:45.862815.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 9==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.3025396392861409\n",
      "============Validation pass of epoch 2 , fold 9 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.98      0.89        55\n",
      "     neutral       0.95      0.85      0.89       285\n",
      "    positive       0.74      0.84      0.79       135\n",
      "\n",
      "    accuracy                           0.86       475\n",
      "   macro avg       0.83      0.89      0.86       475\n",
      "weighted avg       0.87      0.86      0.86       475\n",
      "\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:45.972412.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 9==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.19131307912740245\n",
      "============Validation pass of epoch 3 , fold 9 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      1.00      0.82        55\n",
      "     neutral       0.88      0.92      0.90       285\n",
      "    positive       0.81      0.61      0.69       135\n",
      "\n",
      "    accuracy                           0.84       475\n",
      "   macro avg       0.80      0.84      0.81       475\n",
      "weighted avg       0.84      0.84      0.83       475\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.783263.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 9==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.12046964410970461\n",
      "============Validation pass of epoch 4 , fold 9 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      1.00      0.81        55\n",
      "     neutral       0.89      0.92      0.91       285\n",
      "    positive       0.83      0.61      0.71       135\n",
      "\n",
      "    accuracy                           0.84       475\n",
      "   macro avg       0.80      0.85      0.81       475\n",
      "weighted avg       0.85      0.84      0.84       475\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:46.016090.=============\n",
      "============Fold 9 completed. It took 0:15:07.840347.========\n",
      "=============Fold: 10 of 10 ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 of 4. Fold: 10==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.5690139517632883\n",
      "============Validation pass of epoch 1 , fold 10 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.46      1.00      0.63        55\n",
      "     neutral       0.84      0.78      0.81       285\n",
      "    positive       0.88      0.58      0.70       135\n",
      "\n",
      "    accuracy                           0.75       475\n",
      "   macro avg       0.72      0.79      0.71       475\n",
      "weighted avg       0.81      0.75      0.76       475\n",
      "\n",
      "=========Epoch 1 out of 4 completed. It took 0:03:46.027787.=============\n",
      "\n",
      "======== Epoch 2 of 4. Fold: 10==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.28406009734121723\n",
      "============Validation pass of epoch 2 , fold 10 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.98      0.61        55\n",
      "     neutral       0.87      0.69      0.77       285\n",
      "    positive       0.76      0.71      0.73       135\n",
      "\n",
      "    accuracy                           0.73       475\n",
      "   macro avg       0.69      0.79      0.70       475\n",
      "weighted avg       0.79      0.73      0.74       475\n",
      "\n",
      "=========Epoch 2 out of 4 completed. It took 0:03:45.930171.=============\n",
      "\n",
      "======== Epoch 3 of 4. Fold: 10==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.16823007298438852\n",
      "============Validation pass of epoch 3 , fold 10 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.98      0.64        55\n",
      "     neutral       0.85      0.74      0.79       285\n",
      "    positive       0.79      0.66      0.72       135\n",
      "\n",
      "    accuracy                           0.75       475\n",
      "   macro avg       0.71      0.79      0.72       475\n",
      "weighted avg       0.79      0.75      0.75       475\n",
      "\n",
      "=========Epoch 3 out of 4 completed. It took 0:03:45.889856.=============\n",
      "\n",
      "======== Epoch 4 of 4. Fold: 10==========\n",
      "Training...\n",
      "Batch 30 of 134\n",
      "Batch 60 of 134\n",
      "Batch 90 of 134\n",
      "Batch 120 of 134\n",
      "Training pass done, loss is 0.10740153662471184\n",
      "============Validation pass of epoch 4 , fold 10 completed!============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.98      0.65        55\n",
      "     neutral       0.85      0.76      0.81       285\n",
      "    positive       0.81      0.65      0.72       135\n",
      "\n",
      "    accuracy                           0.76       475\n",
      "   macro avg       0.71      0.80      0.72       475\n",
      "weighted avg       0.80      0.76      0.76       475\n",
      "\n",
      "=========Epoch 4 out of 4 completed. It took 0:03:45.908717.=============\n",
      "============Fold 10 completed. It took 0:15:08.130906.========\n",
      "============TRAINING COMPLETED! It took 2:31:34.512703. ===================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# a list for storing stats concerning the training proccess\n",
    "fold_stats =[]\n",
    "# based on the best macro averaged f1, a model will be saved for use in the application\n",
    "best_macro_f1=0\n",
    "classes=['negative','neutral','positive']\n",
    "fold=0\n",
    "\n",
    "# Generate the stratified 10 fold splits. StartifiedKFold.split returns a generator with the indexes of the split observations and their corresponding labels.\n",
    "N_SPLITS=10\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS)\n",
    "total_time0=datetime.now()\n",
    "for train_index, test_index in skf.split(train_texts, train_labels):\n",
    "  fold_time0=datetime.now()\n",
    "\n",
    "  # Remove the model from the previous fold to avoid running out of GPU memory.\n",
    "  if fold>0:\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "  \n",
    "  print('=============Fold: {} of 10 ============'.format(fold+1))\n",
    "\n",
    "  # Generate the train and test split for the fold.\n",
    "  X_train, X_test = train_texts[train_index], train_texts[test_index]\n",
    "  y_train, y_test = train_labels[train_index], train_labels[test_index]\n",
    "\n",
    "  # Generate the encodings required by BERT \n",
    "  train_encodings= tokenizer.batch_encode_plus(list(X_train),add_special_tokens=True,padding='max_length',max_length=300,return_attention_mask=True)\n",
    "  test_encodings = tokenizer.batch_encode_plus(list(X_test),add_special_tokens=True,padding='max_length',max_length=300,return_attention_mask=True)\n",
    "  # Generate the pytorch datasets\n",
    "  train_dataset=Dataset(train_encodings,list(y_train),list(X_test))\n",
    "  test_dataset= Dataset(test_encodings,list(y_test),list(X_test))\n",
    "  # Generate the pytroch dataloaders\n",
    "  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "  test_loader= DataLoader(test_dataset,batch_size=32)\n",
    "\n",
    "  #Instantiate the model and move it to the GPU (if available).\n",
    "  model = BertForSequenceClassification.from_pretrained('bert-base-cased',num_labels=3)\n",
    "  model.to(device)\n",
    "  #Instantiate the optimizer. Learning rates of 5e-5, 3e-5, and 2e-5 were adhering to what was proposed by Devlin, et al. (2018).\n",
    "  optim = AdamW(model.parameters(), lr=2e-5,eps = 1e-8)\n",
    "\n",
    "  #The paper advises for 2,3, or 4 epochs to be performed.\n",
    "  epochs=4\n",
    "  training_steps = len(train_loader)*epochs\n",
    "  epoch_stats=[]\n",
    "  #A scheduler for reducing the learning rate will be used to aid convergence and counter the potential pitfalls of the large batches.\n",
    "  scheduler = get_linear_schedule_with_warmup(optim,num_warmup_steps=0,num_training_steps=training_steps)\n",
    "\n",
    "  #Start the training \n",
    "  for epoch in range(epochs):\n",
    "    epoch_time0=datetime.now()\n",
    "    print(\"\")\n",
    "    print('======== Epoch {} of {}. Fold: {}=========='.format(epoch + 1, epochs,fold+1))\n",
    "    print('Training...')\n",
    "    \n",
    "    # The below 2 variables are used to calculate the average loss after each training loop is concluded.\n",
    "    total_train_loss=0\n",
    "    avg_train_loss=0\n",
    "    # The predictions variable is needed for generating the classification report.\n",
    "    predictions=np.array([])\n",
    "\n",
    "    #Set the model in a traning stage\n",
    "    model.train()\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Information is printed after every 30 batches were loaded.\n",
    "        if step % 30 == 0 and not step == 0:\n",
    "          print('Batch {} of {}'.format(step,len(train_loader)))\n",
    "\n",
    "        # Get the variables needed from the batch to be passed as parameters to the model .\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        #The model returns the loss using cross entropy loss and the logits(output tensors).\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # Add the loss outputed by the model to the total_train_loss variable for tracking.\n",
    "        loss = outputs[0]\n",
    "        total_train_loss+=loss.item()\n",
    "        \n",
    "\n",
    "        # Perform backpropagation and clip the gradient to avoid the exploding gradiet problem.\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
    "\n",
    "        # Update the parameters using the Adam optimizer, reduce the learning rate using the scheduler and reset all gradients.\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Measure average loss over all batches in the loader    \n",
    "    avg_train_loss=total_train_loss/len(train_loader) \n",
    "    print('Training pass done, loss is {}'.format(avg_train_loss))\n",
    "\n",
    "    # Set the model in a validation stage\n",
    "    model.eval()\n",
    "    # Validation loop\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        #Pytorch automatically computes gradients in its tensors which will not be needed in the validation step\n",
    "        with torch.no_grad():\n",
    "            outputs=model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "        # Get the model outputs\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        # Store the predictions of the model so that they can be used to generate a classificaton report. np.argmax is applied to get the index of the class with highest value. \n",
    "        predictions=np.append(predictions,np.argmax(logits, axis=1).flatten())\n",
    "        \n",
    "    # Append the statistics for the current epoch and extract the macro averaged f1 to be used when deciding whether to save the model.\n",
    "    epoch_stats.append(classification_report(np.array(list(y_test)),predictions,target_names=classes,output_dict=True, zero_division=0))\n",
    "    epoch_macro_f1 =classification_report(np.array(list(y_test)),predictions,target_names=classes,output_dict=True,zero_division=0)['macro avg']['f1-score']\n",
    "\n",
    "    print('============Validation pass of epoch {} , fold {} completed!============'.format(epoch+1,fold+1))\n",
    "    print(classification_report(np.array(list(y_test)),predictions,target_names=classes,zero_division=0))\n",
    "    \n",
    "    # Export the model for further use if it has a higher macro avergared f1 than the previous highest.\n",
    "    if epoch_macro_f1>best_macro_f1:\n",
    "      best_macro_f1=epoch_macro_f1\n",
    "      print('Saving model...')\n",
    "      model.save_pretrained('/content/gdrive/My Drive/BertModel3')\n",
    "      print('Model saved!!')\n",
    "    epoch_time1=datetime.now()-epoch_time0\n",
    "    print('=========Epoch {} out of {} completed. It took {}.============='.format(epoch+1,epochs,str(epoch_time1)))\n",
    "  \n",
    "  fold_time1=datetime.now()-fold_time0\n",
    "  print('============Fold {} completed. It took {}.========'.format(fold+1,str(fold_time1)))\n",
    "  fold_stats.append(epoch_stats)\n",
    "  fold+=1\n",
    "total_time1=datetime.now()-total_time0\n",
    "print('============TRAINING COMPLETED! It took {}. ==================='.format(str(total_time1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt2kPqm7RSXg"
   },
   "source": [
    "Below the reader can see the averaged macro avareraged f1-score across all epochs in all folds. The obtained value was 0.8 which could be regarded as a good indication of the model's performance based on the hyperparameters chosen. From the classification reports after each epoch, the reader can observe that the model seems to exhibit relatively worse perfrormance when guessing negative observations. That is to be expected given that they are the least represented class in the dataset used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrZnSngPSaTR",
    "outputId": "5509e0af-919c-4b3f-e6fc-d318bd30357b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_STEPS=epochs*N_SPLITS\n",
    "macro_avg_f1=0\n",
    "for fold_run in fold_stats:\n",
    "  for epoch_run in fold_run:\n",
    "    macro_avg_f1+=epoch_run['macro avg']['f1-score']\n",
    "final_macro_avg_f1=macro_avg_f1/TOTAL_STEPS\n",
    "round(final_macro_avg_f1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7HPZYIJRwdF"
   },
   "source": [
    "## Exported Model Testing Procedure\n",
    "- The goal of the below testing loop is to validate the exported models' ability to generate on unseen data. The test split of the original dataset is employed for it. Once again, the macro averaged f1-score is the evaluation metric chosen to assess the models' performance. As mentioned the performance of more than one of the exported models will be evaluated. After observing the performance metrics for each model, they are further tested on the custom built news articles dataset to choose a final model for production use (see the 'Selecting Model for Production') notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_St__nM2HclS"
   },
   "outputs": [],
   "source": [
    "# Generate the encodings required by BERT\n",
    "test_encodings = tokenizer.batch_encode_plus(test_texts,add_special_tokens=True,padding='max_length',max_length=300,return_attention_mask=True)\n",
    "# Generate the pytorch datasets\n",
    "test_dataset= Dataset(test_encodings,list(test_labels),list(test_texts))\n",
    "# Generate the pytroch dataloaders\n",
    "test_loader= DataLoader(test_dataset,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtSae2hmPiFH",
    "outputId": "a3166721-bf6c-4f56-fc15-65abb3a2804e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Model: BertModel ===\n",
      "Test Loss: 0.38\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89       408\n",
      "     neutral       0.87      0.92      0.89       563\n",
      "    positive       0.88      0.79      0.83       219\n",
      "\n",
      "    accuracy                           0.88      1190\n",
      "   macro avg       0.88      0.86      0.87      1190\n",
      "weighted avg       0.88      0.88      0.88      1190\n",
      "\n",
      "===Model: BertModel2 ===\n",
      "Test Loss: 0.80\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89       408\n",
      "     neutral       0.85      0.88      0.87       563\n",
      "    positive       0.79      0.75      0.77       219\n",
      "\n",
      "    accuracy                           0.86      1190\n",
      "   macro avg       0.85      0.84      0.84      1190\n",
      "weighted avg       0.86      0.86      0.86      1190\n",
      "\n",
      "===Model: BertModel3 ===\n",
      "Test Loss: 1.78\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.64      0.68       408\n",
      "     neutral       0.67      0.72      0.69       563\n",
      "    positive       0.55      0.59      0.57       219\n",
      "\n",
      "    accuracy                           0.67      1190\n",
      "   macro avg       0.65      0.65      0.65      1190\n",
      "weighted avg       0.67      0.67      0.67      1190\n",
      "\n",
      "===Model: BertFixedPunct ===\n",
      "Test Loss: 2.21\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.85      0.88       408\n",
      "     neutral       0.83      0.91      0.87       563\n",
      "    positive       0.85      0.72      0.78       219\n",
      "\n",
      "    accuracy                           0.86      1190\n",
      "   macro avg       0.86      0.83      0.84      1190\n",
      "weighted avg       0.86      0.86      0.86      1190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing the performance of the saved model from the above training loop.\n",
    "classes=['negative','neutral','positive']\n",
    "# Load the saved model \n",
    "models=['BertModel','BertModel2','BertModel3','BertFixedPunct']\n",
    "path='/content/gdrive/My Drive/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_test_loss=0\n",
    "predictions=np.array([])\n",
    "\n",
    "for bertmodel in models:\n",
    "  model_path=path+bertmodel\n",
    "  model = BertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "  model.eval()\n",
    "  for batch in test_loader:\n",
    "          input_ids = batch['input_ids'].to(device)\n",
    "          attention_mask = batch['attention_mask'].to(device)\n",
    "          labels = batch['labels'].to(device)\n",
    "          with torch.no_grad():\n",
    "              outputs=model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "          total_test_loss += outputs[0].item()\n",
    "          logits = outputs[1].detach().cpu().numpy()\n",
    "          label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "          #store the predictions of the model so that they can be used to generate a classificaton report \n",
    "          predictions=np.append(predictions,np.argmax(logits, axis=1).flatten())\n",
    "    \n",
    "  avg_test_loss = total_test_loss / len(test_loader)   \n",
    "    \n",
    "  print('===Model:',bertmodel,'===')  \n",
    "  print(\"Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "  print('')\n",
    "  print(classification_report(np.array(test_labels),predictions,target_names=classes))\n",
    "  predictions=np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO2cKz4tZpnJ"
   },
   "source": [
    "### Bibliography\n",
    "Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K., 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Google AI Language.\n",
    "\n",
    "Hoffer, E., Hubara, I., & Soudry, D. (2017). Train longer, generalize better: closing the generalization gap in large batch training of neural networks. Advances in Neural Information Processing Systems, 1729-1739.\n",
    "\n",
    "Pytorch., (2021) *Writing Custom Datasets, Dataloaders and Transforms*. Available from: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html [Accessed 15th July 2021]\n",
    "\n",
    "Hugging Face., (2021) *BERT*. Available from: https://huggingface.co/transformers/model_doc/bert.html [Accessed 10th July 2021]\n",
    "\n",
    "Hugging Face., (2021) *BERT Base Model(cased)*. Available from: https://huggingface.co/bert-base-cased [Accessed 10th July 2021]\n",
    "\n",
    "Hugging Face., (2021) *BertForSequenceClassification*. Available from: https://https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification [Accessed 10th July 2021]\n",
    "\n",
    "Hugging Face., (2021) *Tokenizer*. Available from: https://huggingface.co/transformers/main_classes/tokenizer.html [Accessed 11th July 2021]\n",
    "\n",
    "Hugging Face., (2021) *Fine-tuning a pre-trained model*. Available from: https://huggingface.co/transformers/training.html [Accessed 15th July 2021]\n",
    "\n",
    "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "[Malo, P., Sinha, A., Takala, P., Korhonen, P. and Wallenius, J. (2013): “Good debt or bad debt: Detecting semantic orientations in economic texts.” Journal of the American Society for Information Science and Technology. (in Press)](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts)\n",
    "\n",
    "[Sousa, M.G., Sakiyama, K., de Souza Rodrigues, L., Moraes, P.H., Fernandes, E.R. and Matsubara, E.T., 2019, November. BERT for stock market sentiment analysis. In 2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI) (pp. 1597-1601). IEEE.](https://github.com/stocks-predictor/bert)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Fine-Tuning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "33dc6b8de24841318364b3dc93b2c389": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "469ad9498e3e49d49b84f3412e53a4f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51abd8db8f7d4638b72c25c3df51d099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eeba3980337444968a1b5714f59a3910",
      "placeholder": "​",
      "style": "IPY_MODEL_989e6f80a0d94cc6a3873f883d330720",
      "value": " 436M/436M [00:08&lt;00:00, 52.1MB/s]"
     }
    },
    "62b574378c88496e9351cb386c09f547": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97b9c3ddc9bc456a8291bcd625164359",
      "placeholder": "​",
      "style": "IPY_MODEL_c3c1ec160566433794e2789673779bc2",
      "value": "Downloading: 100%"
     }
    },
    "8803f3d2edaa4a6eb3219c48d3f27131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbae98c03303407090ab80e1077b47db",
      "max": 435779157,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_33dc6b8de24841318364b3dc93b2c389",
      "value": 435779157
     }
    },
    "97b9c3ddc9bc456a8291bcd625164359": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "989e6f80a0d94cc6a3873f883d330720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3c1ec160566433794e2789673779bc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dbae98c03303407090ab80e1077b47db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eeba3980337444968a1b5714f59a3910": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb67a4f43df74a7483a57b7b52a3d650": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62b574378c88496e9351cb386c09f547",
       "IPY_MODEL_8803f3d2edaa4a6eb3219c48d3f27131",
       "IPY_MODEL_51abd8db8f7d4638b72c25c3df51d099"
      ],
      "layout": "IPY_MODEL_469ad9498e3e49d49b84f3412e53a4f6"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
